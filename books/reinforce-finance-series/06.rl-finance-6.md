---
title: '強化学習その6: 2.5 General Discussion'
free: true
---
# 強化学習その6: 2.5 General Discussion

この記事は `articles/reinforce-finance.pdf` の **Section 2.5: General Discussion** を、各小節の位置づけがわかる形で整理したメモです。

## 1. 2.5 の全体像

2.5 は、これまでの「無限ホライズン・model-free中心」の議論を拡張して、

1. 有限ホライズンRL
2. 無限ホライズンのmodel-based RL
3. 探索と活用
4. 正則化
5. 非定常環境

を横断的にまとめる節です。

## 2. 2.5.1 RL with Finite Time Horizon

有限ホライズン（episodic）では、1エピソード長 $T$、エピソード数 $K$ として

$$
M=TK
$$

の設定で regret や sample complexity を評価します。  
Section 2.5.1 では model-based / model-free の双方で、UCB 系やPSRL系の結果が整理されています。

金融での意味:

- 最適執行など「明確な終了時刻」がある問題に直結
- 終端コストや在庫制約を自然に組み込める

## 3. 2.5.2 Model-based RL with Infinite Time Horizon

無限ホライズンでも、遷移と報酬を推定して計画する model-based アルゴリズム（R-MAX, MBIE, QVI 等）には理論保証があります。  
また連続状態・連続行動では LQ（LQR, LQ games）構造を使った解析が重要です。

金融での意味:

1. 市場ダイナミクスの近似モデルを持てるなら sample efficiency が上がる可能性
2. ただしモデル誤差に弱く、実務ではロバスト化が必要

## 4. 2.5.3 Exploration-Exploitation Trade-off

Section 2.5.3 で整理される代表手法:

1. $\varepsilon$-greedy
2. UCB
3. Boltzmann / softmax exploration
4. Thompson sampling

Deep RL では、パラメータノイズ・行動ノイズ・entropy 正則化も探索手段として使われます。

金融での意味:

- 探索しすぎると実運用コスト増
- 探索不足だとレジーム変化に追随できない

このバランス設計が最終性能を大きく左右します。

## 5. 2.5.4 Regularization in RL

正則化は、探索改善と学習安定化の両方に効く、というのが 2.5.4 の主張です。

例:

1. TRPO/PPO の KL ペナルティ
2. Soft Q-learning のエントロピー正則化
3. 関数近似でのノルム正則化（過学習抑制）

形式的には、ベルマン作用素や方策改善の目的関数にペナルティ項を足すと

$$
\text{目的関数}
=
\text{期待報酬}
-\lambda\cdot\text{正則化項}
$$

の形になります。

## 6. 2.5.5 Reinforcement Learning in Non-stationary Environment

金融では非定常性が本質なので、2.5.5 は特に重要です。

### 6.1 非定常エピソード設定（式 (2.30)）

エピソード $k$、時刻 $t$ の方策価値:

$$
V_{k,t}^{\Pi}(s)
=
\mathbb{E}^{\Pi}\!\left[
\sum_{u=t}^{T-1}r_{k,u}(s_u,a_u)+r_{k,T}(s_T)
\;\middle|\;
s_t=s
\right]
$$

ここで遷移 $P_{k,t}$、報酬 $r_{k,t}$ は episode/time に依存して変化します。

### 6.2 非定常無限ホライズン設定（式 (2.31)）

$$
V_t^{\Pi}(s)
=
\mathbb{E}^{\Pi}\!\left[
\sum_{u=t}^{\infty}\gamma^{u-t}r_u(s_u,a_u)
\;\middle|\;
s_t=s
\right]
$$

で、$P_u, r_u$ が時刻で変わるケースです。

### 6.3 実務上の含意

1. 既存理論の多くは「非定常度（変化回数など）を既知」と仮定
2. 実務では非定常度は未知なので、適応的アルゴリズムが必要
3. 動的 regret で評価する設計が有効

## 7. まとめ

1. 2.5 は「理論の拡張領域」を整理する節  
2. 金融応用では有限ホライズンと非定常性が特に重要  
3. 探索戦略と正則化が、実装上の安定性・汎化性能を決める

## Python実装ミニ例（非定常バンディット）

前半と後半で最適腕が入れ替わる環境を作り、  
「通常の標本平均更新」と「定数ステップ更新」を比較します。

```python
import numpy as np

T = 2000
means = np.zeros((T, 2))
means[:1000] = [0.2, 0.6]   # 前半: arm 1 が有利
means[1000:] = [0.8, 0.3]   # 後半: arm 0 が有利


def run(sample_average: bool, seed: int, eps: float = 0.1, alpha: float = 0.05):
    rng = np.random.default_rng(seed)
    Q = np.zeros(2)
    N = np.zeros(2)
    regret = np.zeros(T)

    for t in range(T):
        if rng.random() < eps:
            a = int(rng.integers(2))
        else:
            a = int(np.argmax(Q))

        r = float(rng.normal(means[t, a], 0.1))
        if sample_average:
            N[a] += 1.0
            Q[a] += (r - Q[a]) / N[a]
        else:
            Q[a] += alpha * (r - Q[a])

        regret[t] = means[t].max() - means[t, a]

    return np.cumsum(regret)


reg_sa = run(sample_average=True, seed=0)
reg_const = run(sample_average=False, seed=0, alpha=0.05)

print("Dynamic regret (sample-average) =", float(reg_sa[-1]))
print("Dynamic regret (const-step)     =", float(reg_const[-1]))
```

非定常環境では、古いデータを強く残す標本平均より、  
定数ステップで「忘却」を入れる方が追随しやすいことが確認できます。

## 参考

- Ben Hambly, Renyuan Xu, Huining Yang, *Recent Advances in Reinforcement Learning in Finance*, 2023.
- 元メモ: `articles/reinforce-finance.pdf`
