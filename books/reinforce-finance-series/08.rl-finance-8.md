---
title: '強化学習その8: 3.2 Deep Value-based RL Algorithms'
free: true
---
# 強化学習その8: 3.2 Deep Value-based RL Algorithms

この記事は `articles/reinforce-finance.pdf` の **Section 3.2: Deep Value-based RL Algorithms** を、主要アルゴリズムの式に沿って整理したものです。

## 1. 3.2 の全体像

Section 3.2 の主役は次の3つです。

1. Neural Fitted Q-learning
2. Deep Q-Network (DQN)
3. Double DQN

いずれも「$Q$ 関数をニューラルネットで近似し、ベルマン最適方程式の不動点に近づける」方法です。

## 2. Neural Fitted Q-learning

Q関数を

$$
Q(s,a;\theta)=F((s,a);\theta)
$$

で近似します。  
更新ターゲットは

$$
Y_n^Q=r+\gamma\max_{a'}Q(s',a';\theta^{(n)})
$$

（式 (3.3)）で、損失は

$$
L_{\mathrm{NFQ}}(\theta^{(n)})
=
\|Q(s,a;\theta^{(n)})-Y_n^Q\|_2^2
$$

（式 (3.4)）。

パラメータ更新は

$$
\theta^{(n+1)}
=
\theta^{(n)}
+\beta\bigl(Y_n^Q-Q(s,a;\theta^{(n)})\bigr)\nabla_{\theta^{(n)}}Q(s,a;\theta^{(n)})
$$

（式 (3.5)）です。

### 実務上の課題

1. 収束が遅い/不安定になることがある
2. $\max$ による過大評価バイアスが出やすい

## 3. DQN

DQN は上の課題に対して、主に次の2点を導入します。

1. Experience Replay
2. Target Network

### 3.1 Experience Replay

遷移 $(s_t,a_t,r_t,s_{t+1})$ をバッファ $B$ に保存し、ミニバッチをランダムサンプルして学習します。  
時間相関を弱め、勾配推定の分散を下げる効果があります。

### 3.2 Target Network

オンラインネットワーク $\theta$ と、遅く更新するターゲットネットワーク $\theta^-$ を分離します。  
ターゲットは

$$
\tilde Y_i^Q
=
r^{(i)}+\gamma\max_{a'}Q(s'^{(i)},a';\theta^{-(n)})
$$

（式 (3.6)）で、損失は

$$
L_{\mathrm{DQN}}(\theta^{(n)},\theta^{-(n)})
=
\frac{1}{B}\sum_{i=1}^{B}\left(Q(s^{(i)},a^{(i)};\theta^{(n)})-\tilde Y_i^Q\right)^2
$$

（式 (3.7)）です。

ターゲット側は一定間隔で

$$
\theta^- \leftarrow \theta
$$

と同期し、急激な発散を抑えます。

## 4. Double DQN

過大評価バイアスを抑えるため、**行動選択** と **評価** を分離します。

通常の形（比較用）:

$$
Y_n^Q=r+\gamma Q\!\left(s',\arg\max_{a}Q(s',a;\theta^{(n)});\theta^{(n)}\right)
$$

（式 (3.8)）。

Double DQN:

$$
Y_n^Q=r+\gamma Q\!\left(s',\arg\max_{a}Q(s',a;\theta^{(n)});\eta^{(n)}\right)
$$

（式 (3.9)）。

ここで:

- $\theta$: 行動選択（argmax）
- $\eta$: 価値評価

を担当し、過大評価を緩和します。

## 5. 理論結果の位置づけ（3.2 Discussion）

Section 3.2 の理論議論は概ね次です。

1. DQN の誤差分解（統計誤差 + アルゴリズム誤差）
2. 2層NNなど特定設定での非漸近評価
3. 平均二乗サンプル複雑度の上界（サブ線形収束）

代表的な2層ネット表現:

$$
F((s,a);W)
=
\frac{1}{\sqrt m}\sum_{l=1}^{m} c_l\,\sigma(W_l^\top(s,a))
$$

（式 (3.10)）。

## 6. 金融タスクでの実務的解釈

1. DQN系は離散行動（売買段階、執行サイズの離散化）で使いやすい
2. Replay/Target は時系列相関の強い金融データでも安定化に有効
3. ただし非定常環境では replay データの鮮度管理が重要

## Python実装ミニ例（最小 DQN）

line-world 上で replay buffer と target network を使う最小 DQN です。

```python
import random
from collections import deque

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim


class LineWorld:
    def __init__(self, n=5):
        self.n = n
        self.start = n // 2
        self.s = self.start

    def reset(self):
        self.s = self.start
        return self.s

    def step(self, a):
        self.s = max(0, min(self.n - 1, self.s + (-1 if a == 0 else 1)))
        done = self.s in (0, self.n - 1)
        r = -1.0 if self.s == 0 else (1.0 if self.s == self.n - 1 else -0.01)
        return self.s, r, done, {}


def one_hot(s, n):
    x = np.zeros(n, dtype=np.float32)
    x[s] = 1.0
    return x


class QNet(nn.Module):
    def __init__(self, n_states, n_actions):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_states, 32),
            nn.ReLU(),
            nn.Linear(32, n_actions),
        )

    def forward(self, x):
        return self.net(x)


env = LineWorld(n=5)
n_states, n_actions = 5, 2
gamma = 0.95
batch_size = 32
target_interval = 50

q_net = QNet(n_states, n_actions)
target_net = QNet(n_states, n_actions)
target_net.load_state_dict(q_net.state_dict())
opt = optim.Adam(q_net.parameters(), lr=1e-3)
buf = deque(maxlen=5000)

step_count = 0
for ep in range(300):
    s = env.reset()
    done = False
    eps = max(0.05, 0.3 - ep / 1000.0)

    while not done:
        if random.random() < eps:
            a = random.randrange(n_actions)
        else:
            with torch.no_grad():
                qs = q_net(torch.tensor(one_hot(s, n_states)).unsqueeze(0))
                a = int(qs.argmax(dim=1).item())

        ns, r, done, _ = env.step(a)
        buf.append((s, a, r, ns, done))
        s = ns
        step_count += 1

        if len(buf) < batch_size:
            continue

        batch = random.sample(buf, batch_size)
        s_b, a_b, r_b, ns_b, d_b = zip(*batch)

        s_t = torch.tensor([one_hot(x, n_states) for x in s_b], dtype=torch.float32)
        ns_t = torch.tensor([one_hot(x, n_states) for x in ns_b], dtype=torch.float32)
        a_t = torch.tensor(a_b, dtype=torch.long)
        r_t = torch.tensor(r_b, dtype=torch.float32)
        d_t = torch.tensor(d_b, dtype=torch.float32)

        q = q_net(s_t).gather(1, a_t.unsqueeze(1)).squeeze(1)
        with torch.no_grad():
            next_q = target_net(ns_t).max(dim=1).values
            y = r_t + gamma * (1.0 - d_t) * next_q

            # Double DQN にするなら:
            # next_a = q_net(ns_t).argmax(dim=1)
            # next_q = target_net(ns_t).gather(1, next_a.unsqueeze(1)).squeeze(1)
            # y = r_t + gamma * (1.0 - d_t) * next_q

        loss = nn.functional.mse_loss(q, y)
        opt.zero_grad()
        loss.backward()
        opt.step()

        if step_count % target_interval == 0:
            target_net.load_state_dict(q_net.state_dict())


# 学習後の greedy 方策を確認
policy = []
for s in range(n_states):
    with torch.no_grad():
        a = int(q_net(torch.tensor(one_hot(s, n_states)).unsqueeze(0)).argmax(dim=1))
    policy.append(a)
print("greedy policy =", policy)
```

## 参考

- Ben Hambly, Renyuan Xu, Huining Yang, *Recent Advances in Reinforcement Learning in Finance*, 2023.
- 元メモ: `articles/reinforce-finance.pdf`
