---
title: '強化学習その7: 3.1 Neural Networks'
free: true
---
# 強化学習その7: 3.1 Neural Networks

この記事は `articles/reinforce-finance.pdf` の **Section 3.1: Neural Networks** を、RL実装で必要な観点に絞って整理したメモです。

## 1. 3.1 の位置づけ

Section 2 までは tabular/線形近似が中心でした。  
Section 3.1 は、状態や行動が高次元になったときに使う関数近似器として、ニューラルネットの基本形を導入する節です。

主な対象:

1. Fully-connected Neural Networks (FNN)
2. Convolutional Neural Networks (CNN)
3. Recurrent Neural Networks (RNN, LSTM)
4. 学習（SGDとモメンタム）

## 2. Fully-connected Neural Network（式 (3.1)）

層数 $L$、重み $W_l$、バイアス $b_l$、活性化 $\sigma$ に対して、
FNN は

$$
F(z;W,b)=W_L\cdot \sigma\!\left(W_{L-1}\cdots \sigma(W_1z+b_1)\cdots +b_{L-1}\right)+b_L
$$

（式 (3.1)）で表されます。

活性化関数の典型例:

- ReLU: $\sigma(u)=\max(u,0)$
- Leaky ReLU
- $\tanh$

RLではこの $F$ を $Q(s,a;\theta)$ や $\pi(a\mid s;\theta)$ の近似器として使います。

## 3. CNN（畳み込み）

画像・板情報など局所構造を持つ入力では CNN が有効です。  
単一フィルタ $H$ の畳み込みは

$$
[z*H]_{i,j}
=
\sum_{i'=1}^{k_x}\sum_{j'=1}^{k_y}
[z]_{i+i'-1,j+j'-1}[H]_{i',j'}
$$

で与えられ、活性化後の出力は

$$
[\hat z]_{i,j}=\sigma([z*H]_{i,j})
$$

です。  
実際には multi-channel / multi-filter と pooling を組み合わせます。

金融での例:

- 価格系列を画像化した入力
- 板情報（価格帯×数量）テンソル

## 4. RNN / LSTM（時系列向け）

時系列データ $z_1,\dots,z_T$ に対し、RNN は

$$
h_t=F(h_{t-1},z_t;\theta),\qquad
\hat z_t=G(h_{t-1},z_t;\phi)
$$

で内部状態を更新します。

LSTM はゲート機構で長期依存を扱いやすくします。代表形は

$$
\begin{aligned}
f_t&=\sigma(U_f z_t+W_f h_{t-1}+b_f),\\
g_t&=\sigma(U_g z_t+W_g h_{t-1}+b_g),\\
q_t&=\sigma(U_q z_t+W_q h_{t-1}+b_q),\\
c_t&=f_t\odot c_{t-1}+g_t\odot \sigma(U z_t+W h_{t-1}+b),\\
h_t&=\tanh(c_t)\odot q_t.
\end{aligned}
$$

## 5. 学習則（式 (3.2)）

ミニバッチSGDの基本更新は

$$
\theta^{(n+1)}
=
\theta^{(n)}
-\beta\,\widehat{\nabla_\theta L(\theta^{(n)})}
$$

（式 (3.2)）です。

ここで:

- $\beta$: 学習率
- $\widehat{\nabla_\theta L}$: ミニバッチで推定した勾配

モメンタム付き更新（標準/Nesterov）も実務でよく使われます。

## 6. 金融RLでの使い分け

1. FNN: 特徴量ベクトル入力の基本選択
2. CNN: 板情報・画像化入力など局所構造が強い場合
3. RNN/LSTM: 長短期の時系列依存を扱いたい場合

データ構造に合うアーキテクチャを選ぶことが、収束性と汎化性能に直結します。

## Python実装ミニ例（FNN/CNN/LSTM の最小学習）

PyTorch で 3 種類のネットを1ステップだけ学習する確認コードです。

```python
import torch
import torch.nn as nn
import torch.optim as optim

batch = 16
n_actions = 3

# ダミー入力
x_vec = torch.randn(batch, 10)        # FNN用
x_img = torch.randn(batch, 1, 32, 32) # CNN用
x_seq = torch.randn(batch, 20, 8)     # LSTM用 (T=20, feature=8)
target = torch.zeros(batch, dtype=torch.long)

fnn = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, n_actions),
)

cnn = nn.Sequential(
    nn.Conv2d(1, 8, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(8, n_actions),
)

lstm = nn.LSTM(input_size=8, hidden_size=32, batch_first=True)
lstm_head = nn.Linear(32, n_actions)

y_fnn = fnn(x_vec)
y_cnn = cnn(x_img)
h, _ = lstm(x_seq)
y_lstm = lstm_head(h[:, -1, :])

loss_fn = nn.CrossEntropyLoss()
loss = loss_fn(y_fnn, target) + loss_fn(y_cnn, target) + loss_fn(y_lstm, target)

opt = optim.Adam(
    list(fnn.parameters())
    + list(cnn.parameters())
    + list(lstm.parameters())
    + list(lstm_head.parameters()),
    lr=1e-3,
)
opt.zero_grad()
loss.backward()
opt.step()

print("loss =", float(loss))
```

## 参考

- Ben Hambly, Renyuan Xu, Huining Yang, *Recent Advances in Reinforcement Learning in Finance*, 2023.
- 元メモ: `articles/reinforce-finance.pdf`
