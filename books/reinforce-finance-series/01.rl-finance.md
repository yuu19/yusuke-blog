---
title: Finance向けMDPとベルマン方程式の最小整理
free: true
---
# 強化学習その1: Finance向けMDPとベルマン方程式

この記事は、`articles/reinforce-finance.pdf`（Recent Advances in Reinforcement Learning in Finance, 2023）のメモを、単独で読めるように再構成したものです。  
主に PDF の Section 2.1（MDPの定式化）と Section 4（金融応用）をベースにしています。

## この記事でやること

1. 金融タスクをMDPとして書く
2. 価値関数と行動価値関数のベルマン方程式を整理する
3. それが金融応用（執行、ポートフォリオ、ヘッジ）でどう効くかを示す

## 記号と前提

- 状態空間: $S$
- 行動空間: $A$
- 遷移: $s_{t+1}\sim P(\cdot\mid s_t,a_t)$
- 報酬: $R_{t+1}$（または期待報酬 $r(s,a)=\mathbb{E}[R_{t+1}\mid s_t=s,a_t=a]$）
- 割引率: $\gamma\in(0,1)$
- 方策: $\pi(a\mid s)$（確率的方策を許す）

## 1. 無限ホライズンMDP（割引報酬）

金融では、長期運用（例: 長期ポートフォリオ）を無限ホライズンで近似することが多いです。

$$
V^*(s)
=
\sup_{\pi}
\mathbb{E}_{\pi}\!\left[
\sum_{t=0}^{\infty}\gamma^t R_{t+1}
\;\middle|\; s_0=s
\right]
$$

このとき動的計画法（DPP）より、

$$
V^*(s)=\sup_{a\in A}\left\{\mathbb{E}[R_{t+1}\mid s,a]+\gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}[V^*(s')]\right\}
$$

さらに $Q^*(s,a)$ を使うと

$$
V^*(s)=\sup_{a\in A}Q^*(s,a)
$$

$$
Q^*(s,a)=\mathbb{E}[R_{t+1}\mid s,a]+\gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\!\left[\sup_{a'}Q^*(s',a')\right]
$$

となります。

## 2. 有限ホライズンMDP（時間非定常）

短期執行（例: $T$ ステップで在庫をゼロにする）では有限ホライズンが自然です。  
この場合、遷移や報酬は時刻依存（非定常）でもよいです。

$$
V_t^*(s)=
\sup_{\pi}
\mathbb{E}_{\pi}\!\left[
\sum_{u=t}^{T-1}R_{u+1}+R_T^{\mathrm{terminal}}
\;\middle|\; s_t=s
\right]
$$

ベルマン方程式は

$$
V_t^*(s)=\sup_{a\in A}\left\{
\mathbb{E}[R_{t+1}\mid s,a]
\;+\;
\mathbb{E}_{s'\sim P_t(\cdot\mid s,a)}[V_{t+1}^*(s')]
\right\}
$$

です。

## 3. $q_{\pi}$ のベルマン期待方程式

まず Return の定義:

$$
G_t\coloneqq\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}
$$

方策 $\pi$ の行動価値関数は

$$
q_{\pi}(s,a)\coloneqq \mathbb{E}_{\pi}[G_t\mid s_t=s,a_t=a]
$$

です。  
$G_t=R_{t+1}+\gamma G_{t+1}$ を使うと

$$
q_{\pi}(s,a)
=
\mathbb{E}\!\left[
R_{t+1}
+\gamma\,\mathbb{E}_{a'\sim\pi(\cdot\mid s_{t+1})}\!\left[q_{\pi}(s_{t+1},a')\right]
\middle|\,
s_t=s,a_t=a
\right]
$$

遷移確率で展開すると

$$
q_{\pi}(s,a)
=
\sum_{s',r}p(s',r\mid s,a)\left[
r+\gamma\sum_{a'}\pi(a'\mid s')q_{\pi}(s',a')
\right]
$$

が得られます。

## 4. 最適行動価値 $q_*$（ベルマン最適方程式）

方策平均 $\sum_{a'}\pi(a'\mid s')q_{\pi}(s',a')$ を最大化に置き換えると

$$
q_*(s,a)
=
\sum_{s',r}p(s',r\mid s,a)\left[
r+\gamma\max_{a'}q_*(s',a')
\right]
$$

です。  
Q-learning 系はこの不動点方程式をサンプルで近似して解く、という理解で見通しがよくなります。

## 5. 金融タスクとの対応

### 最適執行（Optimal Execution）

- 典型的には有限ホライズン
- 状態: 在庫、価格、板情報、残り時間
- 行動: 注文量、注文方式（成行/指値など）
- 報酬: 執行コストの負値やリスク調整PnL

### ポートフォリオ最適化

- 無限/有限の両方あり
- 状態: 保有比率、価格特徴量、ボラティリティなど
- 行動: リバランス比率
- 報酬: リターンとリスク（分散、CVaR等）のトレードオフ

### オプションヘッジ

- 多くは有限ホライズン（満期あり）
- 状態: 原資産価格、残存時間、Greeks、在庫
- 行動: ヘッジ量
- 報酬: ヘッジ誤差と取引コストを加味した効用

## 6. 実務で重要な注意点

1. 非定常性: 金融データはレジーム変化があり、定常仮定が崩れやすい
2. 報酬設計: PnLだけだと過剰リスクを取ることがある
3. オフライン評価: 反実仮想評価が難しく、過学習しやすい
4. 実行制約: 取引コスト、スリッページ、在庫制約を明示的に入れる必要がある

## まとめ

- 金融RLはまず「MDPとして状態・行動・報酬を定義する」ことから始まる
- 理論の中心はベルマン方程式（期待版と最適版）
- 応用では、定常性よりも「有限時間・制約付き・非定常」への対応が重要

## Python実装ミニ例（ベルマン最適方程式）

次の最小例は、既知の $(P,r)$ に対して value iteration を回し、$V^*$ と greedy 方策を求めます。

```python
import numpy as np

gamma = 0.95
S, A = 3, 2

# P[s, a, s_next]
P = np.array(
    [
        [[0.7, 0.3, 0.0], [0.2, 0.8, 0.0]],
        [[0.0, 0.6, 0.4], [0.0, 0.1, 0.9]],
        [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]],
    ],
    dtype=float,
)

# r(s, a)
R = np.array(
    [
        [0.0, 0.2],
        [0.3, 1.0],
        [0.0, 0.0],
    ],
    dtype=float,
)

V = np.zeros(S)
for it in range(500):
    Q = R + gamma * np.einsum("sat,t->sa", P, V)
    V_next = Q.max(axis=1)
    if np.max(np.abs(V_next - V)) < 1e-10:
        break
    V = V_next

pi_star = Q.argmax(axis=1)
print("iterations =", it + 1)
print("V* =", np.round(V, 6))
print("pi* =", pi_star)
```

## 参考

- Ben Hambly, Renyuan Xu, Huining Yang, *Recent Advances in Reinforcement Learning in Finance*, 2023.
- PDFメモ元: `articles/reinforce-finance.pdf`
