---
title: '強化学習その5: 2.4 Policy-based Methods'
free: true
---
# 強化学習その5: 2.4 Policy-based Methods

この記事は `articles/reinforce-finance.pdf` の **Section 2.4: Policy-based Methods** を、数式の意味が追えるように整理したものです。

## 1. 2.4 の設定

Section 2.4 は「モデルを明示推定せず、方策を直接最適化する」立場です。  
方策をパラメータ $\theta$ で表して

$$
\pi_\theta(a\mid s)
$$

と書きます。

無限ホライズン（定常分布 $\mu^{\pi_\theta}$ を用いる）では目的関数を

$$
J(\theta)\coloneqq \int_S V^{\pi_\theta}(s)\,\mu^{\pi_\theta}(ds)
$$

とおき、更新は

$$
\theta'=\theta+\beta\,\widehat{\nabla_\theta J(\theta)}
$$

（式 (2.27)）です。

## 2. Policy Gradient Theorem（式 (2.28)）

ポイントは、状態分布 $\mu^{\pi_\theta}$ の微分を直接計算しなくても

$$
\nabla_\theta J(\theta)
=
\mathbb{E}_{s\sim\mu^{\pi_\theta},\,a\sim\pi_\theta}
\left[
\nabla_\theta \log \pi_\theta(a\mid s)\,Q^{\pi_\theta}(s,a)
\right]
$$

と表せることです（式 (2.28)）。

この式が「log-derivative trick による方策勾配法」の基礎になります。

## 3. REINFORCE（2.4.2）

REINFORCE は、上式の $Q^{\pi_\theta}$ を実際のリターンで置き換える Monte Carlo 法です。

リターンを

$$
G_t\coloneqq \sum_{u=t+1}^{M}\gamma^{u-t-1}r_u
$$

と置くと、代表的な更新は

$$
\theta \leftarrow \theta+\beta\,\gamma^t G_t\,\nabla_\theta\log \pi_\theta(a_t\mid s_t)
$$

（式 (2.29) の形）になります。

### 3.1 長所と短所

1. 長所: 勾配推定が不偏になりやすい
2. 短所: 分散が大きく、学習が不安定になりやすい

分散削減のため、baseline $b(s)$（典型的には価値関数近似）を用いて

$$
G_t \to G_t-b(s_t)
$$

とするのが標準です。

## 4. Actor-Critic（2.4.3）

Actor-Critic は

- Actor: 方策パラメータ $\theta$ を更新
- Critic: 価値関数（または $Q$）パラメータ $w$ を更新

を同時に行う方法です。

簡易形では TD 誤差

$$
\delta_t
=
r_t+\gamma Q_w(s_{t+1},a_{t+1})-Q_w(s_t,a_t)
$$

を使って

$$
w \leftarrow w+\beta_w\,\delta_t\,\phi(s_t,a_t),
$$

$$
\theta \leftarrow \theta+\beta_\theta\,Q_w(s_t,a_t)\,\nabla_\theta\log \pi_\theta(a_t\mid s_t)
$$

のように更新します（Algorithm 5 の骨子）。

### 4.1 実行形態

1. Nested-loop: critic を内側で十分更新してから actor 更新
2. Two time-scale: actor/critic を別学習率で同時更新
3. Single time-scale: 同時更新だが学習率設計を変える

## 5. 2.4.4 Discussion の要点

Section 2.4.4 では、以下の発展が整理されています。

1. TRPO/PPO: 更新ステップの安定化（KL制約/クリッピング）
2. Natural Policy Gradient: Fisher情報を使う改良勾配
3. DPG/ACER/A2C/A3C: サンプル効率・並列化・連続行動への対応

理論面では

- 漸近収束（stationary point 収束）
- 非漸近サンプル複雑度
- 条件付きでの大域最適性

の3層で結果が積み上がっている、という見方が重要です。

## 6. 金融での読み替え

1. 連続行動（ポジションサイズ等）では policy-based が自然
2. REINFORCE は実装が簡単だが高分散
3. 実務では Actor-Critic（PPO系含む）が安定性と性能のバランスを取りやすい

## Python実装ミニ例（REINFORCE と Actor-Critic）

1状態2行動の bandit を使って、policy gradient 系更新の最小形を比較します。

```python
import numpy as np

rng = np.random.default_rng(1)


def softmax(x: np.ndarray) -> np.ndarray:
    z = x - np.max(x)
    ez = np.exp(z)
    return ez / ez.sum()


def reward(a: int) -> float:
    # action 1 の方が高期待値
    mean = 1.0 if a == 1 else 0.2
    return float(rng.normal(mean, 0.3))


def score_grad(pi: np.ndarray, a: int) -> np.ndarray:
    g = -pi.copy()
    g[a] += 1.0
    return g


# REINFORCE
theta = np.zeros(2)
lr = 0.05
for _ in range(2000):
    pi = softmax(theta)
    a = int(rng.choice(2, p=pi))
    g = score_grad(pi, a)
    r = reward(a)
    theta += lr * r * g
print("REINFORCE pi =", np.round(softmax(theta), 3))

# Actor-Critic (1-step baseline)
theta = np.zeros(2)
v = 0.0
lr_actor = 0.03
lr_critic = 0.05
for _ in range(2000):
    pi = softmax(theta)
    a = int(rng.choice(2, p=pi))
    r = reward(a)
    delta = r - v
    v += lr_critic * delta
    theta += lr_actor * delta * score_grad(pi, a)
print("Actor-Critic pi =", np.round(softmax(theta), 3), "baseline =", round(v, 3))
```

## 参考

- Ben Hambly, Renyuan Xu, Huining Yang, *Recent Advances in Reinforcement Learning in Finance*, 2023.
- 元メモ: `articles/reinforce-finance.pdf`
- Sutton and Barto, *Reinforcement Learning: An Introduction*
